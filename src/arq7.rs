//! Arq 7 data format support
//!
//! This module provides comprehensive support for the Arq 7 backup format, which uses JSON
//! configuration files and a different directory structure compared to earlier versions.
//!
//! ## Features Implemented
//!
//! ### âœ… JSON Configuration Parsing
//! - `BackupConfig` - Parse backupconfig.json files
//! - `BackupFolders` - Parse backupfolders.json files
//! - `BackupPlan` - Parse backupplan.json files
//! - `BackupFolder` - Parse individual folder configurations
//! - `BackupRecord` - Parse LZ4-compressed JSON backup records
//!
//! ### âœ… Complete Backup Set Loading
//! - `BackupSet::from_directory()` - Load entire backup set from directory
//! - Automatic discovery and loading of all JSON configuration files
//! - Recursive loading of backup records from all folders
//! - Error handling with graceful degradation
//!
//! ### ðŸ”„ Binary Format Support (Partial)
//! - Binary format parsing utilities for Nodes and Trees
//! - BlobLoc data loading from pack files
//! - LZ4 decompression support
//! - Foundation for full binary tree/node parsing
//!
//! ## Usage Examples
//!
//! ### Loading a complete backup set:
//! ```rust
//! use arq::arq7::BackupSet;
//!
//! let backup_set = BackupSet::from_directory("/path/to/backup/set")?;
//! println!("Backup name: {}", backup_set.backup_config.backup_name);
//!
//! // Access backup records
//! for (folder_uuid, records) in &backup_set.backup_records {
//!     println!("Folder {}: {} records", folder_uuid, records.len());
//! }
//! ```
//!
//! ### Loading individual components:
//! ```rust
//! use arq::arq7::{BackupConfig, BackupPlan};
//!
//! let config = BackupConfig::from_file("backupconfig.json")?;
//! let plan = BackupPlan::from_file("backupplan.json")?;
//! ```
//!
//! ## Arq 7 Directory Structure
//!
//! ```
//! backup_set_directory/
//! â”œâ”€â”€ backupconfig.json          # Backup configuration
//! â”œâ”€â”€ backupfolders.json         # Object directory locations
//! â”œâ”€â”€ backupplan.json            # Backup plan settings
//! â”œâ”€â”€ backupfolders/             # Per-folder configurations
//! â”‚   â””â”€â”€ <folder-uuid>/
//! â”‚       â”œâ”€â”€ backupfolder.json  # Folder metadata
//! â”‚       â””â”€â”€ backuprecords/     # Backup records by timestamp
//! â”‚           â””â”€â”€ <timestamp>/
//! â”‚               â””â”€â”€ *.backuprecord  # LZ4-compressed JSON records
//! â”œâ”€â”€ blobpacks/                 # Packed blob data
//! â”œâ”€â”€ treepacks/                 # Packed tree data
//! â””â”€â”€ standardobjects/           # Standalone objects
//! ```
//!
//! ## Data Format Notes
//!
//! - JSON files use exact field names (not camelCase transformed)
//! - Backup records are LZ4-compressed JSON with 4-byte big-endian length prefix
//! - Binary pack files contain multiple objects at specific offsets
//! - All timestamps are Unix epoch seconds
//! - UUIDs are used extensively for identifying backup plans, folders, and objects

pub mod binary;

use crate::error::Result;
use byteorder::{BigEndian, ReadBytesExt};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// BackupConfig represents the backupconfig.json file
///
/// This file tells Arq how objects are to be added to the backup set â€“ whether the data are
/// encrypted, what kind of hashing mechanism to use, what maximum size to use for packing
/// small files together, etc.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct BackupConfig {
    /// 1=SHA1, 2=SHA256
    #[serde(rename = "blobIdentifierType")]
    pub blob_identifier_type: u32,
    #[serde(rename = "maxPackedItemLength")]
    pub max_packed_item_length: u64,
    #[serde(rename = "backupName")]
    pub backup_name: String,
    #[serde(rename = "isWORM")]
    pub is_worm: bool,
    #[serde(rename = "containsGlacierArchives")]
    pub contains_glacier_archives: bool,
    #[serde(rename = "additionalUnpackedBlobDirs")]
    pub additional_unpacked_blob_dirs: Vec<String>,
    /// Arq uses the same chunker version to ensure de-duplication works with old data
    #[serde(rename = "chunkerVersion")]
    pub chunker_version: u32,
    #[serde(rename = "computerName")]
    pub computer_name: String,
    #[serde(rename = "computerSerial")]
    pub computer_serial: String,
    #[serde(rename = "blobStorageClass")]
    pub blob_storage_class: String,
    #[serde(rename = "isEncrypted")]
    pub is_encrypted: bool,
}

/// BackupFolders represents the backupfolders.json file
///
/// This file tells Arq where to find existing objects (for de-duplication).
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct BackupFolders {
    #[serde(rename = "standardObjectDirs")]
    pub standard_object_dirs: Vec<String>,
    #[serde(rename = "standardIAObjectDirs")]
    pub standard_ia_object_dirs: Vec<String>,
    #[serde(rename = "onezoneIAObjectDirs")]
    pub onezone_ia_object_dirs: Vec<String>,
    #[serde(rename = "s3GlacierObjectDirs")]
    pub s3_glacier_object_dirs: Vec<String>,
    #[serde(rename = "s3DeepArchiveObjectDirs")]
    pub s3_deep_archive_object_dirs: Vec<String>,
    #[serde(rename = "s3GlacierIRObjectDirs")]
    pub s3_glacier_ir_object_dirs: Vec<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "importedFrom")]
    pub imported_from: Option<String>,
}

/// TransferRate configuration for backup plans
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct TransferRate {
    pub enabled: bool,
    #[serde(rename = "startTimeOfDay")]
    pub start_time_of_day: String,
    #[serde(rename = "daysOfWeek")]
    pub days_of_week: Vec<String>,
    #[serde(rename = "scheduleType")]
    pub schedule_type: String,
    #[serde(rename = "endTimeOfDay")]
    pub end_time_of_day: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "maxKBPS")]
    pub max_kbps: Option<u64>,
}

/// Schedule configuration for backup plans
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Schedule {
    #[serde(rename = "backUpAndValidate")]
    pub backup_and_validate: bool,
    #[serde(rename = "startWhenVolumeIsConnected")]
    pub start_when_volume_is_connected: bool,
    #[serde(rename = "pauseDuringWindow")]
    pub pause_during_window: bool,
    #[serde(rename = "type")]
    pub schedule_type: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "daysOfWeek")]
    pub days_of_week: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "everyHours")]
    pub every_hours: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "minutesAfterHour")]
    pub minutes_after_hour: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "pauseFrom")]
    pub pause_from: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "pauseTo")]
    pub pause_to: Option<String>,
}

/// Email report configuration
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct EmailReport {
    pub port: u32,
    #[serde(rename = "startTLS")]
    pub start_tls: bool,
    #[serde(rename = "authenticationType")]
    pub authentication_type: String,
    #[serde(rename = "reportHELOUseIP")]
    pub report_helo_use_ip: bool,
    pub when: String,
    #[serde(rename = "type")]
    pub report_type: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub hostname: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub username: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "fromAddress")]
    pub from_address: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "toAddress")]
    pub to_address: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub subject: Option<String>,
}

/// BackupFolderPlan represents individual folder configuration within a backup plan
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct BackupFolderPlan {
    #[serde(rename = "backupFolderUUID")]
    pub backup_folder_uuid: String,
    #[serde(rename = "diskIdentifier")]
    pub disk_identifier: String,
    #[serde(rename = "blobStorageClass")]
    pub blob_storage_class: String,
    #[serde(rename = "ignoredRelativePaths")]
    pub ignored_relative_paths: Vec<String>,
    #[serde(rename = "skipIfNotMounted")]
    pub skip_if_not_mounted: bool,
    #[serde(rename = "skipDuringBackup")]
    pub skip_during_backup: bool,
    #[serde(rename = "useDiskIdentifier")]
    pub use_disk_identifier: bool,
    #[serde(rename = "relativePath")]
    pub relative_path: String,
    #[serde(rename = "wildcardExcludes")]
    pub wildcard_excludes: Vec<String>,
    #[serde(rename = "excludedDrives")]
    pub excluded_drives: Vec<String>,
    #[serde(rename = "localPath")]
    pub local_path: String,
    #[serde(rename = "allDrives")]
    pub all_drives: bool,
    #[serde(rename = "skipTMExcludes")]
    pub skip_tm_excludes: bool,
    #[serde(rename = "regexExcludes")]
    pub regex_excludes: Vec<String>,
    pub name: String,
    #[serde(rename = "localMountPoint")]
    pub local_mount_point: String,
}

/// BackupPlan represents the backupplan.json file
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct BackupPlan {
    #[serde(rename = "transferRateJSON")]
    pub transfer_rate_json: TransferRate,
    #[serde(rename = "cpuUsage")]
    pub cpu_usage: u32,
    pub id: u32,
    #[serde(rename = "storageLocationId")]
    pub storage_location_id: u32,
    #[serde(rename = "excludedNetworkInterfaces")]
    pub excluded_network_interfaces: Vec<String>,
    #[serde(rename = "needsArq5Buckets")]
    pub needs_arq5_buckets: bool,
    #[serde(rename = "useBuzhash")]
    pub use_buzhash: bool,
    #[serde(rename = "arq5UseS3IA")]
    pub arq5_use_s3_ia: bool,
    #[serde(rename = "objectLockUpdateIntervalDays")]
    pub object_lock_update_interval_days: u32,
    #[serde(rename = "planUUID")]
    pub plan_uuid: String,
    #[serde(rename = "scheduleJSON")]
    pub schedule_json: Schedule,
    #[serde(rename = "keepDeletedFiles")]
    pub keep_deleted_files: bool,
    pub version: u32,
    #[serde(rename = "createdAtProConsole")]
    pub created_at_pro_console: bool,
    #[serde(rename = "backupFolderPlanMountPointsAreInitialized")]
    pub backup_folder_plan_mount_points_are_initialized: bool,
    #[serde(rename = "includeNewVolumes")]
    pub include_new_volumes: bool,
    #[serde(rename = "retainMonths")]
    pub retain_months: u32,
    #[serde(rename = "useAPFSSnapshots")]
    pub use_apfs_snapshots: bool,
    #[serde(rename = "backupSetIsInitialized")]
    pub backup_set_is_initialized: bool,
    #[serde(rename = "backupFolderPlansByUUID")]
    pub backup_folder_plans_by_uuid: HashMap<String, BackupFolderPlan>,
    #[serde(rename = "notifyOnError")]
    pub notify_on_error: bool,
    #[serde(rename = "retainDays")]
    pub retain_days: u32,
    #[serde(rename = "updateTime")]
    pub update_time: u64,
    #[serde(rename = "excludedWiFiNetworkNames")]
    pub excluded_wi_fi_network_names: Vec<String>,
    #[serde(rename = "objectLockAvailable")]
    pub object_lock_available: bool,
    pub managed: bool,
    pub name: String,
    #[serde(rename = "wakeForBackup")]
    pub wake_for_backup: bool,
    #[serde(rename = "includeNetworkInterfaces")]
    pub include_network_interfaces: bool,
    #[serde(rename = "datalessFilesOption")]
    pub dataless_files_option: u32,
    #[serde(rename = "retainAll")]
    pub retain_all: bool,
    #[serde(rename = "isEncrypted")]
    pub is_encrypted: bool,
    pub active: bool,
    #[serde(rename = "notifyOnSuccess")]
    pub notify_on_success: bool,
    #[serde(rename = "preventSleep")]
    pub prevent_sleep: bool,
    #[serde(rename = "creationTime")]
    pub creation_time: u64,
    #[serde(rename = "pauseOnBattery")]
    pub pause_on_battery: bool,
    #[serde(rename = "retainWeeks")]
    pub retain_weeks: u32,
    #[serde(rename = "retainHours")]
    pub retain_hours: u32,
    #[serde(rename = "preventBackupOnConstrainedNetworks")]
    pub prevent_backup_on_constrained_networks: bool,
    #[serde(rename = "includeWiFiNetworks")]
    pub include_wi_fi_networks: bool,
    #[serde(rename = "threadCount")]
    pub thread_count: u32,
    #[serde(rename = "preventBackupOnExpensiveNetworks")]
    pub prevent_backup_on_expensive_networks: bool,
    #[serde(rename = "emailReportJSON")]
    pub email_report_json: EmailReport,
    #[serde(rename = "includeFileListInActivityLog")]
    pub include_file_list_in_activity_log: bool,
    #[serde(rename = "noBackupsAlertDays")]
    pub no_backups_alert_days: u32,
}

/// BackupFolder represents a backupfolder.json file within the backupfolders/<UUID>/ directory
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct BackupFolder {
    #[serde(rename = "localPath")]
    pub local_path: String,
    #[serde(rename = "migratedFromArq60")]
    pub migrated_from_arq60: bool,
    #[serde(rename = "storageClass")]
    pub storage_class: String,
    #[serde(rename = "diskIdentifier")]
    pub disk_identifier: String,
    pub uuid: String,
    #[serde(rename = "migratedFromArq5")]
    pub migrated_from_arq5: bool,
    #[serde(rename = "localMountPoint")]
    pub local_mount_point: String,
    pub name: String,
}

/// BlobLoc describes the location of a blob
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct BlobLoc {
    #[serde(rename = "blobIdentifier")]
    pub blob_identifier: String,
    #[serde(rename = "compressionType")]
    pub compression_type: u32,
    #[serde(rename = "isPacked")]
    pub is_packed: bool,
    pub length: u64,
    pub offset: u64,
    #[serde(rename = "relativePath")]
    pub relative_path: String,
    #[serde(rename = "stretchEncryptionKey")]
    pub stretch_encryption_key: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "isLargePack")]
    pub is_large_pack: Option<bool>,
}

/// Node describes either a file or a directory in the backup
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Node {
    #[serde(rename = "changeTime_nsec")]
    pub change_time_nsec: u64,
    #[serde(rename = "changeTime_sec")]
    pub change_time_sec: u64,
    #[serde(rename = "computerOSType")]
    pub computer_os_type: u32,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "containedFilesCount")]
    pub contained_files_count: Option<u64>,
    #[serde(rename = "creationTime_nsec")]
    pub creation_time_nsec: u64,
    #[serde(rename = "creationTime_sec")]
    pub creation_time_sec: u64,
    #[serde(rename = "dataBlobLocs")]
    pub data_blob_locs: Vec<BlobLoc>,
    pub deleted: bool,
    #[serde(rename = "isTree")]
    pub is_tree: bool,
    #[serde(rename = "itemSize")]
    pub item_size: u64,
    #[serde(rename = "mac_st_dev")]
    pub mac_st_dev: u64,
    #[serde(rename = "mac_st_flags")]
    pub mac_st_flags: u32,
    #[serde(rename = "mac_st_gid")]
    pub mac_st_gid: u32,
    #[serde(rename = "mac_st_ino")]
    pub mac_st_ino: u64,
    #[serde(rename = "mac_st_mode")]
    pub mac_st_mode: u32,
    #[serde(rename = "mac_st_nlink")]
    pub mac_st_nlink: u32,
    #[serde(rename = "mac_st_rdev")]
    pub mac_st_rdev: u32,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "mac_st_uid")]
    pub mac_st_uid: Option<u32>,
    #[serde(rename = "modificationTime_nsec")]
    pub modification_time_nsec: u64,
    #[serde(rename = "modificationTime_sec")]
    pub modification_time_sec: u64,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "treeBlobLoc")]
    pub tree_blob_loc: Option<BlobLoc>,
    #[serde(rename = "winAttrs")]
    pub win_attrs: u32,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "xattrsBlobLocs")]
    pub xattrs_blob_locs: Option<Vec<BlobLoc>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub username: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "groupName")]
    pub group_name: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "reparseTag")]
    pub reparse_tag: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "reparsePointIsDirectory")]
    pub reparse_point_is_directory: Option<bool>,
}

/// BackupRecord represents a backup record file containing backup metadata and the root node
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct BackupRecord {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub archived: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "arqVersion")]
    pub arq_version: Option<String>,
    #[serde(rename = "backupFolderUUID")]
    pub backup_folder_uuid: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "backupPlanJSON")]
    pub backup_plan_json: Option<BackupPlan>,
    #[serde(rename = "backupPlanUUID")]
    pub backup_plan_uuid: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "computerOSType")]
    pub computer_os_type: Option<u32>,
    #[serde(rename = "copiedFromCommit")]
    pub copied_from_commit: bool,
    #[serde(rename = "copiedFromSnapshot")]
    pub copied_from_snapshot: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "creationDate")]
    pub creation_date: Option<u64>,
    #[serde(rename = "diskIdentifier")]
    pub disk_identifier: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "errorCount")]
    pub error_count: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "isComplete")]
    pub is_complete: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "localMountPoint")]
    pub local_mount_point: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "localPath")]
    pub local_path: Option<String>,
    pub node: Node,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "relativePath")]
    pub relative_path: Option<String>,
    #[serde(rename = "storageClass")]
    pub storage_class: String,
    pub version: u32,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "volumeName")]
    pub volume_name: Option<String>,
    #[serde(rename = "backupRecordErrors")]
    pub backup_record_errors: Vec<String>,
}

/// BackupSet represents an entire Arq 7 backup set
#[derive(Debug, Clone)]
pub struct BackupSet {
    pub backup_config: BackupConfig,
    pub backup_folders: BackupFolders,
    pub backup_plan: BackupPlan,
    pub backup_folder_configs: HashMap<String, BackupFolder>,
    pub backup_records: HashMap<String, Vec<BackupRecord>>,
}

impl BackupConfig {
    /// Load a BackupConfig from a JSON reader
    pub fn from_reader<R: std::io::Read>(reader: R) -> Result<Self> {
        Ok(serde_json::from_reader(reader)?)
    }

    /// Load a BackupConfig from a file path
    pub fn from_file<P: AsRef<std::path::Path>>(path: P) -> Result<Self> {
        let file = std::fs::File::open(path)?;
        Self::from_reader(file)
    }
}

impl BackupFolders {
    /// Load BackupFolders from a JSON reader
    pub fn from_reader<R: std::io::Read>(reader: R) -> Result<Self> {
        Ok(serde_json::from_reader(reader)?)
    }

    /// Load BackupFolders from a file path
    pub fn from_file<P: AsRef<std::path::Path>>(path: P) -> Result<Self> {
        let file = std::fs::File::open(path)?;
        Self::from_reader(file)
    }
}

impl BackupPlan {
    /// Load a BackupPlan from a JSON reader
    pub fn from_reader<R: std::io::Read>(reader: R) -> Result<Self> {
        Ok(serde_json::from_reader(reader)?)
    }

    /// Load a BackupPlan from a file path
    pub fn from_file<P: AsRef<std::path::Path>>(path: P) -> Result<Self> {
        let file = std::fs::File::open(path)?;
        Self::from_reader(file)
    }
}

impl BackupFolder {
    /// Load a BackupFolder from a JSON reader
    pub fn from_reader<R: std::io::Read>(reader: R) -> Result<Self> {
        Ok(serde_json::from_reader(reader)?)
    }

    /// Load a BackupFolder from a file path
    pub fn from_file<P: AsRef<std::path::Path>>(path: P) -> Result<Self> {
        let file = std::fs::File::open(path)?;
        Self::from_reader(file)
    }
}

impl Node {
    /// Load the tree data if this node is a tree
    pub fn load_tree(
        &self,
        backup_set_path: &std::path::Path,
    ) -> Result<Option<binary::BinaryTree>> {
        if let Some(tree_blob_loc) = &self.tree_blob_loc {
            // Convert JSON BlobLoc to binary BlobLoc format for loading
            let binary_blob_loc = BlobLoc {
                blob_identifier: tree_blob_loc.blob_identifier.clone(),
                compression_type: tree_blob_loc.compression_type,
                is_packed: tree_blob_loc.is_packed,
                length: tree_blob_loc.length,
                offset: tree_blob_loc.offset,
                relative_path: tree_blob_loc.relative_path.clone(),
                stretch_encryption_key: tree_blob_loc.stretch_encryption_key,
                is_large_pack: tree_blob_loc.is_large_pack,
            };

            Ok(Some(tree_blob_loc.load_tree(backup_set_path)?))
        } else {
            Ok(None)
        }
    }

    /// Convert a binary::BinaryNode to a Node for recursive traversal
    pub fn from_binary_node(binary_node: &binary::BinaryNode) -> Self {
        Node {
            change_time_nsec: binary_node.ctime_nsec as u64,
            change_time_sec: binary_node.ctime_sec as u64,
            computer_os_type: binary_node.computer_os_type,
            contained_files_count: Some(binary_node.contained_files_count),
            creation_time_nsec: binary_node.create_time_nsec as u64,
            creation_time_sec: binary_node.create_time_sec as u64,
            data_blob_locs: binary_node
                .data_blob_locs
                .iter()
                .map(|b| BlobLoc::from_binary_blob_loc(b))
                .collect(),
            deleted: binary_node.deleted,
            is_tree: binary_node.is_tree,
            item_size: binary_node.item_size,
            mac_st_dev: binary_node.mac_st_dev as u64,
            mac_st_flags: binary_node.mac_st_flags as u32,
            mac_st_gid: binary_node.mac_st_gid,
            mac_st_ino: binary_node.mac_st_ino,
            mac_st_mode: binary_node.mac_st_mode,
            mac_st_nlink: binary_node.mac_st_nlink,
            mac_st_rdev: binary_node.mac_st_rdev as u32,
            mac_st_uid: Some(binary_node.mac_st_uid),
            modification_time_nsec: binary_node.mtime_nsec as u64,
            modification_time_sec: binary_node.mtime_sec as u64,
            tree_blob_loc: binary_node
                .tree_blob_loc
                .as_ref()
                .map(|b| BlobLoc::from_binary_blob_loc(b)),
            win_attrs: binary_node.win_attrs,
            xattrs_blob_locs: Some(
                binary_node
                    .xattrs_blob_locs
                    .iter()
                    .map(|b| BlobLoc::from_binary_blob_loc(b))
                    .collect(),
            ),
            username: binary_node.username.clone(),
            group_name: binary_node.group_name.clone(),
            reparse_tag: binary_node.win_reparse_tag,
            reparse_point_is_directory: binary_node.win_reparse_point_is_directory,
        }
    }
}

impl BackupSet {
    /// Load a complete BackupSet from a directory path
    pub fn from_directory<P: AsRef<std::path::Path>>(path: P) -> Result<Self> {
        let path = path.as_ref();

        // Load main configuration files
        let backup_config = BackupConfig::from_file(path.join("backupconfig.json"))?;
        let backup_folders = BackupFolders::from_file(path.join("backupfolders.json"))?;
        let backup_plan = BackupPlan::from_file(path.join("backupplan.json"))?;

        // Load backup folder configurations
        let mut backup_folder_configs = HashMap::new();
        let mut backup_records = HashMap::new();
        let backupfolders_dir = path.join("backupfolders");

        if backupfolders_dir.exists() {
            for entry in std::fs::read_dir(backupfolders_dir)? {
                let entry = entry?;
                if entry.file_type()?.is_dir() {
                    let folder_uuid = entry.file_name().to_string_lossy().to_string();
                    let config_path = entry.path().join("backupfolder.json");

                    if config_path.exists() {
                        let backup_folder = BackupFolder::from_file(config_path)?;
                        backup_folder_configs.insert(folder_uuid.clone(), backup_folder);
                    }

                    // Load backup records for this folder
                    let records_dir = entry.path().join("backuprecords");
                    if records_dir.exists() {
                        let mut folder_records = Vec::new();
                        Self::load_backup_records_recursive(&records_dir, &mut folder_records)?;
                        if !folder_records.is_empty() {
                            backup_records.insert(folder_uuid, folder_records);
                        }
                    }
                }
            }
        }

        Ok(BackupSet {
            backup_config,
            backup_folders,
            backup_plan,
            backup_folder_configs,
            backup_records,
        })
    }

    /// Recursively load backup record files from a directory
    fn load_backup_records_recursive(
        dir: &std::path::Path,
        records: &mut Vec<BackupRecord>,
    ) -> Result<()> {
        for entry in std::fs::read_dir(dir)? {
            let entry = entry?;
            let path = entry.path();

            if entry.file_type()?.is_dir() {
                // Recursively search subdirectories
                Self::load_backup_records_recursive(&path, records)?;
            } else if path.extension().and_then(|s| s.to_str()) == Some("backuprecord") {
                // Try to parse backup record file
                match BackupRecord::from_file(&path) {
                    Ok(record) => records.push(record),
                    Err(e) => {
                        // Log error but continue processing other files
                        eprintln!("Warning: Failed to parse backup record {:?}: {}", path, e);
                    }
                }
            }
        }
        Ok(())
    }
}

impl BackupRecord {
    /// Load a BackupRecord from a file path
    /// The file format is: 4-byte big-endian length + LZ4-compressed JSON data
    pub fn from_file<P: AsRef<std::path::Path>>(path: P) -> Result<Self> {
        let mut file = std::fs::File::open(path)?;
        Self::from_reader(&mut file)
    }

    /// Load a BackupRecord from a reader
    pub fn from_reader<R: std::io::Read>(mut reader: R) -> Result<Self> {
        use byteorder::{BigEndian, ReadBytesExt};
        use std::io::Read;

        // Read the 4-byte decompressed length header
        let decompressed_length = reader.read_u32::<BigEndian>()?;

        // Read all remaining compressed data
        let mut compressed_data = Vec::new();
        reader.read_to_end(&mut compressed_data)?;

        // Decompress using LZ4 with known decompressed size
        let decompressed = lz4_flex::decompress(&compressed_data, decompressed_length as usize)?;

        // Parse JSON
        let record: BackupRecord = serde_json::from_slice(&decompressed)?;
        Ok(record)
    }
}

impl BlobLoc {
    /// Convert a binary::BinaryBlobLoc to a BlobLoc
    pub fn from_binary_blob_loc(binary_blob: &binary::BinaryBlobLoc) -> Self {
        BlobLoc {
            blob_identifier: binary_blob.blob_identifier.clone(),
            compression_type: binary_blob.compression_type,
            is_packed: binary_blob.is_packed,
            length: binary_blob.length,
            offset: binary_blob.offset,
            relative_path: binary_blob.relative_path.clone(),
            stretch_encryption_key: binary_blob.stretch_encryption_key,
            is_large_pack: None, // Not available in binary format
        }
    }

    /// Load the actual blob data from a pack file or standalone object
    pub fn load_data(&self, backup_set_path: &std::path::Path) -> Result<Vec<u8>> {
        // Handle different relative path formats:
        // 1. JSON format: "/FD5575D9-B7E1-43D9-B29C-B54ACC9BC2A9/treepacks/..."
        // 2. Binary format paths: "/FD5575D9-B7E1-43D9-B29C-B54ACC9BC2A9/blobpacks/..."
        let blob_path = {
            // Handle normal paths with UUID prefix - strip the UUID part
            let path_parts: Vec<&str> = self.relative_path.split('/').collect();
            let path_without_uuid = if path_parts.len() > 2 && !path_parts[1].is_empty() {
                // Skip the UUID part (first non-empty component)
                path_parts[2..].join("/")
            } else {
                // Fallback to removing just the leading slash
                self.relative_path.trim_start_matches('/').to_string()
            };
            backup_set_path.join(&path_without_uuid)
        };

        self.load_from_pack_file(&blob_path)
    }

    /// Load data from a specific pack file
    fn load_from_pack_file(&self, blob_path: &std::path::Path) -> Result<Vec<u8>> {
        use std::io::{Read, Seek};

        if self.is_packed {
            // Load from pack file using exact offset and length
            let mut file = std::fs::File::open(blob_path)?;
            file.seek(std::io::SeekFrom::Start(self.offset))?;

            let mut buffer = vec![0u8; self.length as usize];
            let bytes_read = file.read(&mut buffer)?;
            buffer.truncate(bytes_read);

            // Handle blob pack format based on compression type
            if self.compression_type == 2 {
                // LZ4 format: [4-byte decompressed length][LZ4 compressed data]
                if buffer.len() >= 4 {
                    let decompressed_length =
                        u32::from_be_bytes([buffer[0], buffer[1], buffer[2], buffer[3]]) as usize;

                    // LZ4 compressed data starts after the 4-byte length header
                    let compressed_data = &buffer[4..];

                    // Decompress using LZ4
                    match lz4_flex::decompress(compressed_data, decompressed_length) {
                        Ok(decompressed) => return Ok(decompressed),
                        Err(_) => {
                            // If LZ4 decompression fails, fall back to treating as raw data
                            // This handles edge cases where data might not be properly compressed
                            if decompressed_length <= buffer.len() - 4 {
                                return Ok(buffer[4..4 + decompressed_length].to_vec());
                            }
                        }
                    }
                }
            } else if self.compression_type == 0 {
                // Uncompressed format: [4-byte content length][raw data]
                if buffer.len() >= 4 {
                    let content_length =
                        u32::from_be_bytes([buffer[0], buffer[1], buffer[2], buffer[3]]) as usize;

                    // Raw data starts after the 4-byte length header
                    if content_length <= buffer.len() - 4 {
                        return Ok(buffer[4..4 + content_length].to_vec());
                    }
                }
            }

            // Fallback - return the raw buffer if we can't parse the format
            Ok(buffer)
        } else {
            // Load standalone object
            let data = std::fs::read(blob_path)?;

            // Handle compression
            if self.compression_type == 2 {
                // LZ4 compression with 4-byte length prefix
                if data.len() >= 4 {
                    let decompressed_length =
                        u32::from_be_bytes([data[0], data[1], data[2], data[3]]);
                    let decompressed =
                        lz4_flex::decompress(&data[4..], decompressed_length as usize)?;
                    Ok(decompressed)
                } else {
                    Ok(data)
                }
            } else {
                Ok(data)
            }
        }
    }

    /// Load and parse a tree from this blob location
    pub fn load_tree(&self, backup_set_path: &std::path::Path) -> Result<binary::BinaryTree> {
        let data = self.load_data(backup_set_path)?;

        // For standalone objects, load_data already handles decompression
        let mut cursor = std::io::Cursor::new(&data);
        binary::BinaryTree::from_reader(&mut cursor)
    }

    /// Load and parse a node from this blob location
    pub fn load_node(
        &self,
        backup_set_path: &std::path::Path,
        tree_version: Option<u32>,
    ) -> Result<binary::BinaryNode> {
        let data = self.load_data(backup_set_path)?;

        // Parse as LZ4-compressed binary node
        let mut cursor = std::io::Cursor::new(&data);
        let decompressed_length = cursor.read_u32::<BigEndian>()?;

        let compressed_data = &data[4..];
        let decompressed = lz4_flex::decompress(compressed_data, decompressed_length as usize)?;

        let mut decompressed_cursor = std::io::Cursor::new(decompressed);
        binary::BinaryNode::from_reader(&mut decompressed_cursor, tree_version)
    }

    /// Extract the actual file content from this blob location
    pub fn extract_content(&self, backup_set_path: &std::path::Path) -> Result<Vec<u8>> {
        self.load_data(backup_set_path)
    }

    /// Extract file content as a UTF-8 string (for text files)
    pub fn extract_text_content(&self, backup_set_path: &std::path::Path) -> Result<String> {
        let content = self.extract_content(backup_set_path)?;
        Ok(String::from_utf8_lossy(&content).to_string())
    }

    /// Save extracted content to a file
    pub fn extract_to_file<P: AsRef<std::path::Path>>(
        &self,
        backup_set_path: &std::path::Path,
        output_path: P,
    ) -> Result<()> {
        let content = self.extract_content(backup_set_path)?;
        std::fs::write(output_path, content)?;
        Ok(())
    }
}

impl Node {
    /// Get real blob locations from this node (for files)
    pub fn get_data_blob_locations(&self) -> &[BlobLoc] {
        &self.data_blob_locs
    }

    /// Get tree blob location (for directories)
    pub fn get_tree_blob_location(&self) -> Option<&BlobLoc> {
        self.tree_blob_loc.as_ref()
    }
}

impl BackupSet {
    /// Find real blob locations for files in the backup records
    /// This can be used when binary parsing produces fake blob paths
    pub fn find_real_blob_locations(&self) -> Vec<(String, String)> {
        let mut blob_locations = Vec::new();

        for records in self.backup_records.values() {
            for record in records {
                self.collect_blob_locations_from_node(
                    &record.node,
                    String::new(),
                    &mut blob_locations,
                );
            }
        }

        blob_locations
    }

    /// Recursively collect blob locations from a node tree
    fn collect_blob_locations_from_node(
        &self,
        node: &Node,
        path: String,
        blob_locations: &mut Vec<(String, String)>,
    ) {
        // Collect data blob locations (for files)
        for blob_loc in &node.data_blob_locs {
            blob_locations.push((path.clone(), blob_loc.relative_path.clone()));
        }

        // If this is a tree, try to load and traverse children
        if node.is_tree {
            if let Some(tree_blob_loc) = &node.tree_blob_loc {
                blob_locations.push((
                    format!("{}/tree", path),
                    tree_blob_loc.relative_path.clone(),
                ));
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_binary_tree_loading() {
        // Test loading a tree from binary data (if pack files exist)
        let backup_set_dir =
            std::path::Path::new("tests/arq_storage_location/FD5575D9-B7E1-43D9-B54ACC9BC2A9");

        if let Ok(backup_set) = BackupSet::from_directory(backup_set_dir) {
            if let Some(records) = backup_set
                .backup_records
                .get("29F6E502-2737-4417-8023-4940D61BA375")
            {
                if let Some(record) = records.first() {
                    // Try to load the tree referenced by the root node
                    match record.node.load_tree(backup_set_dir) {
                        Ok(Some(_tree)) => {
                            // Successfully loaded binary tree data
                            println!("Successfully loaded binary tree data");
                        }
                        _ => {
                            // Tree loading failed, which is expected if pack files don't exist
                            println!("Tree loading failed (expected if pack files don't exist)");
                        }
                    }
                }
            }
        }
    }

    #[test]
    fn test_parse_backup_config() {
        let json = r#"{
            "blobIdentifierType": 2,
            "maxPackedItemLength": 256000,
            "backupName": "Back up to arq_storage_location",
            "isWORM": false,
            "containsGlacierArchives": false,
            "additionalUnpackedBlobDirs": [],
            "chunkerVersion": 3,
            "computerName": "Lars's MacBook Pro",
            "computerSerial": "unused",
            "blobStorageClass": "STANDARD",
            "isEncrypted": false
        }"#;

        let config: BackupConfig = serde_json::from_str(json).unwrap();
        assert_eq!(config.blob_identifier_type, 2);
        assert_eq!(config.backup_name, "Back up to arq_storage_location");
        assert_eq!(config.computer_name, "Lars's MacBook Pro");
        assert!(!config.is_encrypted);
    }

    #[test]
    fn test_parse_backup_folder() {
        let json = r#"{
            "localPath": "/arq/arq_backup_source",
            "migratedFromArq60": false,
            "storageClass": "STANDARD",
            "diskIdentifier": "ROOT",
            "uuid": "29F6E502-2737-4417-8023-4940D61BA375",
            "migratedFromArq5": false,
            "localMountPoint": "/",
            "name": "arq_backup_source"
        }"#;

        let folder: BackupFolder = serde_json::from_str(json).unwrap();
        assert_eq!(folder.local_path, "/arq/arq_backup_source");
        assert_eq!(folder.uuid, "29F6E502-2737-4417-8023-4940D61BA375");
        assert_eq!(folder.name, "arq_backup_source");
        assert!(!folder.migrated_from_arq5);
    }

    #[test]
    fn test_lz4_decompression() {
        // Test that we can decompress LZ4 data (using lz4_flex directly)
        let original = b"Hello, World! This is a test string for LZ4 compression.";
        let compressed = lz4_flex::compress_prepend_size(original);
        let decompressed = lz4_flex::decompress_size_prepended(&compressed).unwrap();
        assert_eq!(original, decompressed.as_slice());
    }

    #[test]
    fn test_backup_record_parsing() {
        // Test that backup records can be parsed from the test data
        let record_path = std::path::Path::new("tests/arq_storage_location/FD5575D9-B7E1-43D9-B29C-B54ACC9BC2A9/backupfolders/29F6E502-2737-4417-8023-4940D61BA375/backuprecords/00173/6107191.backuprecord");

        if record_path.exists() {
            let record = BackupRecord::from_file(record_path).unwrap();

            // Verify basic record structure
            assert_eq!(
                record.backup_folder_uuid,
                "29F6E502-2737-4417-8023-4940D61BA375"
            );
            assert_eq!(
                record.backup_plan_uuid,
                "FD5575D9-B7E1-43D9-B29C-B54ACC9BC2A9"
            );
            assert_eq!(record.disk_identifier, "ROOT");
            assert_eq!(record.storage_class, "STANDARD");
            assert_eq!(record.version, 100);
            assert!(!record.copied_from_commit);
            assert!(!record.copied_from_snapshot);
            assert!(record.backup_record_errors.is_empty());

            // Verify node structure
            assert!(record.node.is_tree);
            assert_eq!(record.node.computer_os_type, 1);
            assert!(!record.node.deleted);
            assert!(record.node.tree_blob_loc.is_some());

            // Verify arq version if present
            if let Some(arq_version) = &record.arq_version {
                assert!(arq_version.starts_with("7."));
            }
        }
    }
}
